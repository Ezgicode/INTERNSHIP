# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YpB3SoDEzamW_jRNBV67l6A5pQf-UQGu
"""

!pip install transformers datasets
!pip install -U transformers

import pandas as pd

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from datasets import Dataset

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

train=pd.read_csv("train.csv")
test=pd.read_csv("test.csv")

train.columns= ["tür","başlık","açıklama"]
test.columns=["tür","başlık","açıklama"]
text_train=train["başlık"]
label_train=train["tür"]-1
text_test=test["başlık"]
label_test=test["tür"]-1

train_dataset=Dataset.from_pandas(train)
test_dataset=Dataset.from_pandas(test)

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased",num_labels=4)

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col.startswith("input") or col.startswith("attention")])
test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if col.startswith("input") or col.startswith("attention")])

def tokenize_data(dataset):
  return tokenizer(dataset["başlık"], truncation=True,padding="max_length", return_tensors="pt", return_attention_mask=True)

tokenized_train=train_dataset.map(tokenize_data,batched=True)
tokenized_test=test_dataset.map(tokenize_data,batched=True)

from transformers import TrainingArguments
training_args= TrainingArguments(
    output_dir="bert_result",
    per_device_train_batch_size=32,
    num_train_epochs=3,
    report_to="none"

)

from transformers import Trainer
trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test
)

trainer.train()

trainer.save_model("bert_ara_model")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Tahminleri al
tahmin = trainer.predict(tokenized_test)

# Argmax ile en yüksek olasılığı seç (tahmin.label_ids ile karşılaştırmak için)
y_pred = np.argmax(tahmin.predictions, axis=1)
y_true = tahmin.label_ids

# Confusion matrix ve classification report
print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred, target_names=["World", "Sport", "Business", "Sci_Fic"]))

tokenized_train = tokenized_train.add_column("labels", label_train.tolist())
tokenized_test = tokenized_test.add_column("labels", label_test.tolist())

trainer.train()

from transformers import Trainer
trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test
)