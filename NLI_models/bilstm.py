# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OvlFcpFS2aCR65izyfDh5icA1HWzq5lH
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense,Dropout

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

data_dev = pd.read_csv("/content/snli_1.0_dev.txt",
                       encoding="utf-8", sep="\t", on_bad_lines="skip")
data_train = pd.read_csv("/content/snli_1.0_test.txt",
                         encoding="utf-8", sep="\t", on_bad_lines="skip")

data_dev = data_dev[["gold_label", "sentence1", "sentence2"]]
data_train = data_train[["gold_label", "sentence1", "sentence2"]]

label_map = {'entailment': 0, 'neutral': 1, 'contradiction': 2}
data_dev["label"] = data_dev["gold_label"].map(label_map)
data_train["label"] = data_train["gold_label"].map(label_map)

data_dev = data_dev.dropna(subset=["label", "sentence1", "sentence2"])
data_train = data_train.dropna(subset=["label", "sentence1", "sentence2"])

data_dev["combined"] = data_dev["sentence1"] + " " + data_dev["sentence2"]
data_train["combined"] = data_train["sentence1"] + " " + data_train["sentence2"]

vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(data_train["combined"])
X_dev_tfidf = vectorizer.transform(data_dev["combined"])
y_train = data_train["label"].values
y_dev = data_dev["label"].values

# TOKENIZE OLAYI
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(data_train["combined"])
X_train_seq = tokenizer.texts_to_sequences(data_train["combined"])
X_dev_seq = tokenizer.texts_to_sequences(data_dev["combined"])
max_len = 80
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')
X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_len, padding='post', truncating='post')

print(" BiLSTM Modeli ")
model = Sequential([
    Embedding(input_dim=10000, output_dim=64, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()
model.fit(X_train_pad,y_train, validation_data=(X_dev_pad, y_dev), epochs=10, batch_size=128)


pred_probs = model.predict(X_dev_pad)
pred_bilstm = np.argmax(pred_probs, axis=1)

print("Accuracy:", accuracy_score(y_dev, pred_bilstm))
print("F1 Score:", f1_score(y_dev, pred_bilstm, average='weighted'))
print("Confusion Matrix:\n", confusion_matrix(y_dev, pred_bilstm))
print("Classification Report:\n", classification_report(y_dev, pred_bilstm, labels=[0, 1, 2], target_names=['entailment','neutral','contradiction']))
